{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shuo-Zh/dl/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT 预训练和微调"
      ],
      "metadata": {
        "id": "BeJBkgDpUNpK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZt9yVh1i1hz"
      },
      "source": [
        "- BERT 前：NLP中的迁移学习 -- word2vec(替换embedding层)；但word2vec 忽略了时序信息\n",
        "- BERT 动机：基于微调的NLP模型；预训练模型抽取了足够多的信息；新任务只需要增加一个简单的输出层"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX_ZAf-ojuCo"
      },
      "source": [
        "## 只有编码器的 Transformer（只有encoder）\n",
        "\n",
        "1.   Base: #blocks = 12; hidden size = 768, #heads =12, #parameters = 110M\n",
        "2.   Large: #blocks = 24, hidden size =1024, #heads= 16，#parameters = 340M\n",
        "3. 大规模数据上训练 > 3B 词\n",
        "\n",
        "\"<cls>\" this moive is great \"<sep>\" i like it \"<sep>\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRNXZ8S3lcjq"
      },
      "source": [
        "预训练任务1:带掩码的语言模型\n",
        "Transformer 编码器是双向，标准语言模型要求单向\n",
        "待掩码的语言模型每次随机（15%概率）将一些词元换成 <\\mask>\n",
        "\n",
        "---\n",
        "因为微调任务中不出现 <\\mask>\n",
        "*   80% 概率下，将选中词元变成 <\\mask>\n",
        "*   10% 概率下换成一个随机词元\n",
        "*   10% 概率 保存原有词元\n",
        "\n",
        "---\n",
        "预训练任务2：下一句子预测\n",
        "于此一个句子对中两个句子是不是相邻\n",
        "训练样本中：\n",
        "*   50%概率选择相邻句子对\n",
        "*   50%概率选择随机句子对\n",
        "\n",
        "将</cls>对应输出放到一个全连接层来预测\n",
        "\n",
        "---\n",
        "- 在微调期间，不同应用之间的 BERT 所需的“最小架构更改”是额外的全连接层\n",
        "- 在下游应用的监督学习期间，额外的参数是从零开始学习的，而预训练 BERT 模型中的所有参数都是微调的\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 文本分类\n",
        "- 特殊分类标记 <cls> 用于序列分类\n",
        "- 特殊分类标记 <sep> 用于标记单个文本的结束或者分隔成对文本"
      ],
      "metadata": {
        "id": "-uKeeL-b8Cl2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qV4X-ytoQxn"
      },
      "source": [
        "# 实现 -> 数据 -> 训练"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvFV1mx-QDqn",
        "outputId": "00c06cfc-c57e-4d4c-be3e-2c1baa15db0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.10/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from d2l) (1.23.5)\n",
            "Requirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.10/dist-packages (from d2l) (3.7.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.10/dist-packages (from d2l) (0.1.6)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from d2l) (2.31.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from d2l) (2.0.3)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from d2l) (1.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.5.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (5.4.4)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (9.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline==0.1.6->d2l) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.3.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.5)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.8)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.9.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.5.7)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.0.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.19.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l) (3.10.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (0.2.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (4.19.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.10.2)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.6.2)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (1.15.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9YgzGODoNe6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import d2l\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, max_len = 512, 64\n",
        "train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJUuGxl7TacZ",
        "outputId": "dbae3688-d8c5-4779-9047-557a1c9125e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ../data/wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2个句子变成 Bert 输入"
      ],
      "metadata": {
        "id": "ZfswwumZU6WR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "432WVcE6oZsV"
      },
      "outputs": [],
      "source": [
        "# Input Representation,\n",
        "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
        "    \"\"\"获取输入序列的词元及其片段索引\"\"\"\n",
        "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
        "    # 0和1分别标记片段A和B\n",
        "    segments = [0] * (len(tokens_a) + 2)\n",
        "    if tokens_b is not None:\n",
        "        tokens += tokens_b + ['<sep>']\n",
        "        segments += [1] * (len(tokens_b) + 1)\n",
        "    return tokens, segments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert做encoder\n"
      ],
      "metadata": {
        "id": "bUGXHdEkU13D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "    \"\"\"残差连接后进行层规范化\"\"\"\n",
        "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
        "        super(AddNorm, self).__init__(**kwargs)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(normalized_shape)\n",
        "\n",
        "    def forward(self, X, Y):\n",
        "        return self.ln(self.dropout(Y) + X)"
      ],
      "metadata": {
        "id": "U0NhYaXNXREb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_norm = AddNorm([3, 4], 0.5)\n",
        "add_norm.eval()\n",
        "add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqKoGNleXWKn",
        "outputId": "01e85353-f7be-42fd-f26f-70b3b96bceb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFFN(nn.Module):\n",
        "    \"\"\"基于位置的前馈网络\"\"\"\n",
        "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n",
        "                 **kwargs):\n",
        "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
        "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.dense2(self.relu(self.dense1(X)))"
      ],
      "metadata": {
        "id": "VJiAfQ1VXeRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 多头自注意力和基于位置的前馈网络\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"Transformer编码器块\"\"\"\n",
        "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
        "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
        "                 dropout, use_bias=False, **kwargs):\n",
        "        super(EncoderBlock, self).__init__(**kwargs)\n",
        "        self.attention = d2l.MultiHeadAttention(\n",
        "             num_hiddens, num_heads, dropout,\n",
        "            use_bias)\n",
        "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
        "        self.ffn = PositionWiseFFN(\n",
        "            ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
        "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
        "\n",
        "    def forward(self, X, valid_lens):\n",
        "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
        "        return self.addnorm2(Y, self.ffn(Y))"
      ],
      "metadata": {
        "id": "DGN6BDKzK-ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4QqA8_foPKY"
      },
      "outputs": [],
      "source": [
        "class BERTEncoder(nn.Module):\n",
        "    \"\"\"BERT编码器\"\"\"\n",
        "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
        "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
        "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
        "                 **kwargs):\n",
        "        super(BERTEncoder, self).__init__(**kwargs)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
        "        # 增加\n",
        "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_layers): # 多少个 block\n",
        "            self.blks.add_module(f\"{i}\", EncoderBlock(\n",
        "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
        "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
        "        # 在BRT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\n",
        "        # pos_embedding，随机初始化下\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
        "                                                      num_hiddens))\n",
        "\n",
        "    # token embedding; positional embedding;\n",
        "    def forward(self, tokens, segments, valid_lens):\n",
        "        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）\n",
        "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
        "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
        "        for blk in self.blks:\n",
        "            X = blk(X, valid_lens)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zytJz3jMpE1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c60624-59e2-4ec5-dd4b-4a9544a8db5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "# Inference of BertEncoder 创建实例、初始化参数\n",
        "# 假设词表大小为10000，为了演示BERTEncoder的前向推断，让我们创建一个实例并初始化它的参数。\n",
        "\n",
        "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
        "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
        "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
        "                      ffn_num_hiddens, num_heads, num_layers, dropout)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1N6az2CpSFG",
        "outputId": "ff270ee3-a0d3-4a33-e002-d5343ac05e42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 8, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# 我们将tokens定义为长度为8的2个输入序列，其中每个词元是词表的索引。\n",
        "# 使用输入tokens的BERTEncoder的前向推断返回编码结果，其中每个词元由向量表示，其长度由超参数num_hiddens定义。\n",
        "# 此超参数通常称为Transformer编码器的隐藏大小（隐藏单元数）。\n",
        "tokens = torch.randint(0, vocab_size, (2, 8))\n",
        "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
        "encoded_X = encoder(tokens, segments, None)\n",
        "encoded_X.shape # batch_size = 2; length = 8; num_hiddens = 768;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRMP6gvhpdWH"
      },
      "source": [
        "# BERT 做预测\n",
        "## 预测方式1：Masked Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRqVUMBgpU16"
      },
      "outputs": [],
      "source": [
        "# Masked Language Modeling\n",
        "class MaskLM(nn.Module):\n",
        "    \"\"\"BERT的掩蔽语言模型任务\"\"\"\n",
        "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
        "        super(MaskLM, self).__init__(**kwargs)\n",
        "        self.mlp = nn.Sequential( #承接 masked_X\n",
        "            nn.Linear(num_inputs, num_hiddens),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(num_hiddens),\n",
        "            nn.Linear(num_hiddens, vocab_size)) #\n",
        "\n",
        "    # X Bert 输出；pred_position, 哪些地方加了 mask\n",
        "    def forward(self, X, pred_positions): #每一个句子中，第0个句子预测第0个token，预计 Bert 位置特征拎出来\n",
        "        num_pred_positions = pred_positions.shape[1]\n",
        "        pred_positions = pred_positions.reshape(-1)\n",
        "        batch_size = X.shape[0]\n",
        "        batch_idx = torch.arange(0, batch_size)\n",
        "        # 假设batch_size=2，num_pred_positions=3\n",
        "        # 那么batch_idx是np.array（[0,0,0,1,1,1]）\n",
        "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
        "        masked_X = X[batch_idx, pred_positions] # 输出对应token下的特征，后丢入MLP\n",
        "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
        "        mlm_Y_hat = self.mlp(masked_X)\n",
        "        return mlm_Y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsdU03ZpphDL",
        "outputId": "a31ad792-ed79-4565-8845-d3d10105ba80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# forward inference of MaskMLM\n",
        "\n",
        "# 将mlm_positions定义为在encoded_X的任一输入序列中预测的3个指示。\n",
        "# mlm的前向推断返回encoded_X的所有掩蔽位置mlm_positions处的预测结果mlm_Y_hat。\n",
        "# 对于每个预测，结果的大小等于词表的大小。\n",
        "mlm = MaskLM(vocab_size, num_hiddens)\n",
        "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]]) #第一个；第五个；第二个； 第六个；第一个；第五个；\n",
        "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
        "mlm_Y_hat.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7mVG2JkqG60",
        "outputId": "c0706a63-b4b2-4d9e-c7fe-47b59d440e1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#通过掩码下的预测词元mlm_Y的真实标签mlm_Y_hat，我们可以计算在BERT预训练中的遮蔽语言模型任务的交叉熵损失。\n",
        "# cross entropy loss\n",
        "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
        "loss = nn.CrossEntropyLoss(reduction='none')\n",
        "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
        "mlm_l.shape\n",
        "# 6 = 2 * 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSk81DgQqJe0"
      },
      "source": [
        "# BERT 做预测\n",
        "## 预测方式2：Next Sentenct Pridiction\n",
        "- 单分类问题"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1_WyEQ2qOAh"
      },
      "outputs": [],
      "source": [
        "class NextSentencePred(nn.Module):\n",
        "    \"\"\"BERT的下一句预测任务\"\"\"\n",
        "    def __init__(self, num_inputs, **kwargs):\n",
        "        super(NextSentencePred, self).__init__(**kwargs)\n",
        "        self.output = nn.Linear(num_inputs, 2) #output = 2\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X的形状：(batchsize,num_hiddens)\n",
        "        return self.output(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksPX3iHAqQq_",
        "outputId": "1825249a-3e4f-462a-8137-c23e3de934cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# The forward inference of an NextSentencePred\n",
        "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
        "# NSP的输入形状:(batchsize，num_hiddens)\n",
        "nsp = NextSentencePred(encoded_X.shape[-1])\n",
        "nsp_Y_hat = nsp(encoded_X)\n",
        "nsp_Y_hat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dDr6YAHqhXY"
      },
      "source": [
        "计算两个二元分类的交叉熵损失"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tST0yX-3qizb",
        "outputId": "93516fbb-2247-4208-d522-3de20274f036"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "nsp_y = torch.tensor([0, 1])\n",
        "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
        "nsp_l.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4NJaUs8qnCU"
      },
      "source": [
        "#整合代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK94T5ypqpTd"
      },
      "outputs": [],
      "source": [
        "class BERTModel(nn.Module):\n",
        "    \"\"\"BERT模型\"\"\"\n",
        "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
        "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
        "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
        "                 hid_in_features=768, mlm_in_features=768,\n",
        "                 nsp_in_features=768):\n",
        "        super(BERTModel, self).__init__()\n",
        "        #encoder, mlm, nsp 三模型\n",
        "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n",
        "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
        "                    dropout, max_len=max_len, key_size=key_size,\n",
        "                    query_size=query_size, value_size=value_size)\n",
        "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
        "                                    nn.Tanh())\n",
        "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
        "        self.nsp = NextSentencePred(nsp_in_features)\n",
        "\n",
        "    def forward(self, tokens, segments, valid_lens=None,\n",
        "                pred_positions=None):\n",
        "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
        "        if pred_positions is not None:\n",
        "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
        "        else:\n",
        "            mlm_Y_hat = None\n",
        "        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引\n",
        "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :])) #0 把分类token对应向量抽出来做二分类问题\n",
        "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDCIOBijrOsi"
      },
      "source": [
        "# Bert 预训练数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fbquF3Xy2_v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H6tHgQkrTs0"
      },
      "outputs": [],
      "source": [
        "# WikiText-2 dataset\n",
        "d2l.DATA_HUB['wikitext-2'] = (\n",
        "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
        "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
        "\n",
        "def _read_wiki(data_dir):\n",
        "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
        "    with open(file_name, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # 大写字母转换为小写字母\n",
        "    paragraphs = [line.strip().lower().split(' . ')\n",
        "                  for line in lines if len(line.split(' . ')) >= 2]\n",
        "    random.shuffle(paragraphs)\n",
        "    return paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6IZeq_Fstd4"
      },
      "outputs": [],
      "source": [
        "# genearating the next sentence prediction task\n",
        "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
        "    if random.random() < 0.5:\n",
        "        is_next = True\n",
        "    else:\n",
        "        # paragraphs是三重列表的嵌套\n",
        "        next_sentence = random.choice(random.choice(paragraphs))\n",
        "        is_next = False\n",
        "    return sentence, next_sentence, is_next\n",
        "\n",
        "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
        "    nsp_data_from_paragraph = []\n",
        "    for i in range(len(paragraph) - 1):\n",
        "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
        "            paragraph[i], paragraph[i + 1], paragraphs)\n",
        "        # 考虑1个'<cls>'词元和2个'<sep>'词元\n",
        "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
        "            continue\n",
        "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
        "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
        "    return nsp_data_from_paragraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8tTTTu8tIjp"
      },
      "outputs": [],
      "source": [
        "# 抽出token，随机换成 mask\n",
        "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n",
        "                        vocab):\n",
        "    # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元\n",
        "    mlm_input_tokens = [token for token in tokens]\n",
        "    pred_positions_and_labels = []\n",
        "    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
        "    random.shuffle(candidate_pred_positions)\n",
        "    for mlm_pred_position in candidate_pred_positions:\n",
        "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
        "            break\n",
        "        masked_token = None\n",
        "        # 80%的时间：将词替换为“<mask>”词元\n",
        "        if random.random() < 0.8:\n",
        "            masked_token = '<mask>'\n",
        "        else:\n",
        "            # 10%的时间：保持词不变\n",
        "            if random.random() < 0.5:\n",
        "                masked_token = tokens[mlm_pred_position] #本身\n",
        "            # 10%的时间：用随机词替换该词\n",
        "            else:\n",
        "                masked_token = random.choice(vocab.idx_to_token)\n",
        "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
        "        pred_positions_and_labels.append(\n",
        "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
        "    return mlm_input_tokens, pred_positions_and_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-m9I85xz8aO"
      },
      "outputs": [],
      "source": [
        "def _get_mlm_data_from_tokens(tokens, vocab):\n",
        "    candidate_pred_positions = []\n",
        "    # tokens是一个字符串列表\n",
        "    for i, token in enumerate(tokens):\n",
        "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
        "        if token in ['<cls>', '<sep>']:\n",
        "            continue\n",
        "        candidate_pred_positions.append(i)\n",
        "    # 遮蔽语言模型任务中预测15%的随机词元\n",
        "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
        "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
        "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
        "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
        "                                       key=lambda x: x[0])\n",
        "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
        "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
        "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels] #position存下来"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZRG6cxStY59"
      },
      "outputs": [],
      "source": [
        "# Append the special \"<mask>\" tokens into input\n",
        "def _pad_bert_inputs(examples, max_len, vocab):\n",
        "    max_num_mlm_preds = round(max_len * 0.15)\n",
        "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
        "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
        "    nsp_labels = []\n",
        "    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n",
        "         is_next) in examples:\n",
        "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n",
        "            max_len - len(token_ids)), dtype=torch.long))\n",
        "        all_segments.append(torch.tensor(segments + [0] * (\n",
        "            max_len - len(segments)), dtype=torch.long))\n",
        "        # valid_lens不包括'<pad>'的计数\n",
        "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
        "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
        "            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
        "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
        "        all_mlm_weights.append(\n",
        "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
        "                max_num_mlm_preds - len(pred_positions)),\n",
        "                dtype=torch.float32))\n",
        "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n",
        "            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
        "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
        "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
        "            all_mlm_weights, all_mlm_labels, nsp_labels) #weights=1：合法两个句子"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mepvVyQqtkOY"
      },
      "outputs": [],
      "source": [
        "# the wikiText-2 dataset for pretraining BERT\n",
        "class _WikiTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, paragraphs, max_len):\n",
        "        # 输入paragraphs[i]是代表段落的句子字符串列表；\n",
        "        # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表\n",
        "        paragraphs = [d2l.tokenize(\n",
        "            paragraph, token='word') for paragraph in paragraphs]\n",
        "        sentences = [sentence for paragraph in paragraphs\n",
        "                     for sentence in paragraph]\n",
        "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "            '<pad>', '<mask>', '<cls>', '<sep>']) # special vocab\n",
        "        # 获取下一句子预测任务的数据\n",
        "        examples = []\n",
        "        for paragraph in paragraphs:\n",
        "            examples.extend(_get_nsp_data_from_paragraph(\n",
        "                paragraph, paragraphs, self.vocab, max_len))\n",
        "        # 获取遮蔽语言模型任务的数据\n",
        "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
        "                      + (segments, is_next))\n",
        "                     for tokens, segments, is_next in examples]\n",
        "        # 填充输入\n",
        "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
        "         self.all_pred_positions, self.all_mlm_weights,\n",
        "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n",
        "            examples, max_len, self.vocab)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
        "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
        "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
        "                self.nsp_labels[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_L3CtM1tkjB"
      },
      "outputs": [],
      "source": [
        "def load_data_wiki(batch_size, max_len):\n",
        "    \"\"\"加载WikiText-2数据集\"\"\"\n",
        "    num_workers = d2l.get_dataloader_workers()\n",
        "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
        "    paragraphs = _read_wiki(data_dir)\n",
        "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
        "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
        "                                        shuffle=True, num_workers=num_workers)\n",
        "    return train_iter, train_set.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFeCG7oztxkS",
        "outputId": "70bd794a-a55c-4c42-e058-2b81d4148663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
          ]
        }
      ],
      "source": [
        "# 重要！！\n",
        "batch_size, max_len = 512, 64\n",
        "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
        "\n",
        "# 所有 token [512, 64], segment [512, 64], valid_lens:[512], pred_position:[512, 10],\n",
        "# weight[512,10], nsp_y [512]\n",
        "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
        "     mlm_Y, nsp_y) in train_iter:\n",
        "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
        "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
        "          nsp_y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvCdyzZir9Gj",
        "outputId": "15944eb0-729b-4a6a-8c8f-fd48959ddcf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20256"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHmiNboc2j0C"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# sentences = 'This is great ! Why not ?'\n",
        "# nltk(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhg494DKukDc"
      },
      "source": [
        "# BERT 预训练代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxWpCatYt1UD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMTSuWZPsZvI"
      },
      "outputs": [],
      "source": [
        "batch_size, max_len = 512, 64\n",
        "train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBvjZ9-3sd5s"
      },
      "outputs": [],
      "source": [
        "# 定义 Small BERT，2 layers， 128 hidden units（每个词抽取长为128特征），2 self-attention heads\n",
        "net = d2l.BERTModel(len(vocab), num_hiddens=128, ffn_num_hiddens=256, num_heads=2,\n",
        "                    dropout=0.2, num_blks=2)\n",
        "\n",
        "# net = d2l.BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],\n",
        "#                     ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,\n",
        "#                     num_layers=2, dropout=0.2, key_size=128, query_size=128,\n",
        "#                     value_size=128, hid_in_features=128, mlm_in_features=128,\n",
        "#                     nsp_in_features=128)\n",
        "devices = d2l.try_all_gpus() ## 使用 GPU\n",
        "loss = nn.CrossEntropyLoss() ## 分类模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybjcQny5siaE"
      },
      "outputs": [],
      "source": [
        "# compute the loss for both the masked language modeling and next sentence prediction tasks\n",
        "# 计算 LOSS\n",
        "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
        "                         segments_X, valid_lens_x,\n",
        "                         pred_positions_X, mlm_weights_X,\n",
        "                         mlm_Y, nsp_y):\n",
        "    # 前向传播\n",
        "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
        "                                  valid_lens_x.reshape(-1),\n",
        "                                  pred_positions_X)\n",
        "    # 计算遮蔽语言模型损失\n",
        "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
        "    mlm_weights_X.reshape(-1, 1) # 被pad部分不算loss\n",
        "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
        "    # 计算下一句子预测任务的损失\n",
        "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
        "    l = mlm_l + nsp_l # 两个loss相加，batch的loss\n",
        "    return mlm_l, nsp_l, l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pcsk2HQesnNP"
      },
      "outputs": [],
      "source": [
        "# Pretrain BERT(net) on the WikiText-2 dataset，唯一换的是 data_iterator, net\n",
        "\n",
        "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
        "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "    step, timer = 0, d2l.Timer()\n",
        "    animator = d2l.Animator(xlabel='step', ylabel='loss',\n",
        "                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
        "    # 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数\n",
        "    metric = d2l.Accumulator(4)\n",
        "    num_steps_reached = False\n",
        "    from matplotlib_inline import backend_inline\n",
        "    backend_inline.set_matplotlib_formats('png')\n",
        "    while step < num_steps and not num_steps_reached:\n",
        "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
        "            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
        "            tokens_X = tokens_X.to(devices[0])\n",
        "            segments_X = segments_X.to(devices[0])\n",
        "            valid_lens_x = valid_lens_x.to(devices[0])\n",
        "            pred_positions_X = pred_positions_X.to(devices[0])\n",
        "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
        "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
        "            trainer.zero_grad()\n",
        "            timer.start()\n",
        "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
        "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
        "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
        "            timer.stop()\n",
        "            animator.add(step + 1,\n",
        "                         (metric[0] / metric[3], metric[1] / metric[3]))\n",
        "            step += 1\n",
        "            if step == num_steps:\n",
        "                num_steps_reached = True\n",
        "                break\n",
        "\n",
        "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
        "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
        "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
        "          f'{str(devices)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "LOnt9P_0s8i1",
        "outputId": "a2345384-c091-4a74-a101-892dad7060e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLM loss 5.259, NSP loss 0.761\n",
            "3235.5 sentence pairs/sec on [device(type='cuda', index=0)]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_bert(train_iter, net, loss, len(vocab), devices, 50)\n",
        "# 50->10W 能用的模型，因此不收敛\n",
        "# 50 batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTU4KncVy9Yg"
      },
      "source": [
        "### 用BERT表示文本（Representing Text with BERT）"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "d2l.DATA_HUB['SNLI'] = (\n",
        "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
        "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
        "\n",
        "data_dir = d2l.download_extract('SNLI')"
      ],
      "metadata": {
        "id": "VNh1Lmm2RC7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKO36Ucdy8qJ"
      },
      "outputs": [],
      "source": [
        "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
        "    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
        "    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n",
        "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
        "    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n",
        "    encoded_X, _, _ = net(token_ids, segments, valid_len) # BERT 抽出特征 encoded_X\n",
        "    return encoded_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qHssEa4vGIe",
        "outputId": "aca9e72d-4df0-493d-e78f-9453c01c5753"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 6, 128]),\n",
              " torch.Size([1, 128]),\n",
              " tensor([0.1750, 0.6349, 0.7913], device='cuda:0', grad_fn=<SliceBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "tokens_a = ['a', 'crane', 'is', 'flying']\n",
        "\n",
        "encoded_text = get_bert_encoding(net, tokens_a)\n",
        "# 词元：'<cls>','a','crane','is','flying','<sep>'\n",
        "encoded_text_cls = encoded_text[:, 0, :]\n",
        "encoded_text_crane = encoded_text[:, 2, :]\n",
        "encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhFfgib-zIWE",
        "outputId": "6845f9bb-6a9d-4f92-c617-46bf2226be33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 10, 128]),\n",
              " torch.Size([1, 128]),\n",
              " tensor([0.1538, 0.6941, 0.8549], device='cuda:0', grad_fn=<SliceBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# 句子对（\"a crane drvier came\" and \"he just left\"）\n",
        "tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']\n",
        "encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n",
        "# 词元：'<cls>','a','crane','driver','came','<sep>','he','just',\n",
        "# 'left','<sep>'\n",
        "encoded_pair_cls = encoded_pair[:, 0, :]\n",
        "encoded_pair_crane = encoded_pair[:, 2, :]\n",
        "encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_snli(data_dir, is_train):\n",
        "    \"\"\"将SNLI数据集解析为前提、假设和标签\"\"\"\n",
        "    def extract_text(s):\n",
        "        # 删除我们不会使用的信息\n",
        "        s = re.sub('\\\\(', '', s)\n",
        "        s = re.sub('\\\\)', '', s)\n",
        "        # 用一个空格替换两个或多个连续的空格\n",
        "        s = re.sub('\\\\s{2,}', ' ', s)\n",
        "        return s.strip()\n",
        "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
        "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n",
        "                             if is_train else 'snli_1.0_test.txt')\n",
        "    with open(file_name, 'r') as f:\n",
        "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
        "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
        "    hypotheses = [extract_text(row[2]) for row in rows if row[0] \\\n",
        "                in label_set]\n",
        "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
        "    return premises, hypotheses, labels"
      ],
      "metadata": {
        "id": "_aKWYE1mQ0J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = read_snli(data_dir, is_train=True)\n",
        "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
        "    print('前提：', x0)\n",
        "    print('假设：', x1)\n",
        "    print('标签：', y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4STdT9SwR9Md",
        "outputId": "bab971fa-e6a2-40fe-c2e0-3aaa7d841df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "前提： A person on a horse jumps over a broken down airplane .\n",
            "假设： A person is training his horse for a competition .\n",
            "标签： 2\n",
            "前提： A person on a horse jumps over a broken down airplane .\n",
            "假设： A person is at a diner , ordering an omelette .\n",
            "标签： 1\n",
            "前提： A person on a horse jumps over a broken down airplane .\n",
            "假设： A person is outdoors , on a horse .\n",
            "标签： 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c78d7uFY5AOs",
        "outputId": "8fe7062d-b528-430a-a0ce-658ef90e1795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[183416, 183187, 182764]\n",
            "[3368, 3237, 3219]\n"
          ]
        }
      ],
      "source": [
        "#Labels \"entailment\", \"contradiction\", and \"neutral\" are balanced\n",
        "# 无论句子、句子对，都可以有 128 维特征\n",
        "test_data = read_snli(data_dir, is_train=False)\n",
        "for data in [train_data, test_data]:\n",
        "    print([[row for row in data[2]].count(i) for i in range(3)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRrj3iHr5JeA"
      },
      "source": [
        "loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmWvUMph5F7M"
      },
      "outputs": [],
      "source": [
        "class SNLIDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"A customized dataset to load the SNLI dataset.\"\"\"\n",
        "    def __init__(self, dataset, num_steps, vocab=None):\n",
        "        self.num_steps = num_steps\n",
        "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
        "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
        "        if vocab is None: # vocab 必须用pretrain dataset\n",
        "        #构建从0开始训练，把vocab弄进来就可以预训练\n",
        "            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n",
        "                                   min_freq=5, reserved_tokens=['<pad>'])\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "        self.premises = self._pad(all_premise_tokens)\n",
        "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
        "        self.labels = torch.tensor(dataset[2])\n",
        "        print('read ' + str(len(self.premises)) + ' examples')\n",
        "\n",
        "    def _pad(self, lines):\n",
        "        return torch.tensor([d2l.truncate_pad(\n",
        "            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n",
        "                         for line in lines])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.premises)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay1XkVgH5h7T",
        "outputId": "81cde198-6d45-40cf-da7f-abcef01507c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read 549367 examples\n",
            "read 9824 examples\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18678"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "def load_data_snli(batch_size, num_steps=50):\n",
        "    \"\"\"Download the SNLI dataset and return data iterators and vocabulary.\"\"\"\n",
        "    num_workers = d2l.get_dataloader_workers()\n",
        "    data_dir = d2l.download_extract('SNLI')\n",
        "    train_data = read_snli(data_dir, True)\n",
        "    test_data = read_snli(data_dir, False)\n",
        "    train_set = SNLIDataset(train_data, num_steps)\n",
        "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
        "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=num_workers)\n",
        "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
        "                                            shuffle=False,\n",
        "                                            num_workers=num_workers)\n",
        "    return train_iter, test_iter, train_set.vocab\n",
        "\n",
        "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvlXzJpB5MFm",
        "outputId": "d66feb2b-a102-4310-c65c-b286c4b96513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 50])\n",
            "torch.Size([128, 50])\n",
            "torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "for X, Y in train_iter:\n",
        "    print(X[0].shape)\n",
        "    print(X[1].shape)\n",
        "    print(Y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT 微调 - 准备自然语言推理数据集"
      ],
      "metadata": {
        "id": "XNTrVqf94jf1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDpmlOcd17qL"
      },
      "source": [
        "使用 BERT 微调只需要增加输出层，但根据任务不同，输入的表示，使用的 BERT 特征也会不一样"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BfTOlSUFtth",
        "outputId": "78b249f8-5d2d-4365-9065-50e26d18113c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.10/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from d2l) (1.23.5)\n",
            "Requirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.10/dist-packages (from d2l) (3.7.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.10/dist-packages (from d2l) (0.1.6)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from d2l) (2.31.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from d2l) (2.0.3)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from d2l) (1.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.5.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (5.4.4)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (9.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline==0.1.6->d2l) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.3.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.5)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.8)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.9.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.5.7)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.0.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.19.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l) (3.10.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (0.2.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (4.19.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.10.2)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.6.2)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (1.15.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPG5xAhPzKum"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "d2l.DATA_HUB['SNLI'] = (\n",
        "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
        "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
        "\n",
        "data_dir = d2l.download_extract('SNLI')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7pyJbWq468N"
      },
      "outputs": [],
      "source": [
        "def read_snli(data_dir, is_train):\n",
        "    \"\"\"Read the SNLI dataset into premises, hypotheses, and labels.\"\"\"\n",
        "    def extract_text(s):\n",
        "        s = re.sub('\\\\(', '', s)\n",
        "        s = re.sub('\\\\)', '', s)\n",
        "        s = re.sub('\\\\s{2,}', ' ', s)\n",
        "        return s.strip()\n",
        "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
        "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n",
        "                             if is_train else 'snli_1.0_test.txt')\n",
        "    with open(file_name, 'r') as f:\n",
        "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
        "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
        "    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n",
        "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
        "    return premises, hypotheses, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go8ir9jG4-l7",
        "outputId": "a247043e-640f-43a5-9c9c-f10529cce01d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is training his horse for a competition .\n",
            "label: 2\n",
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is at a diner , ordering an omelette .\n",
            "label: 1\n",
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is outdoors , on a horse .\n",
            "label: 0\n"
          ]
        }
      ],
      "source": [
        "train_data = read_snli(data_dir, is_train=True)\n",
        "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
        "    print('premise:', x0)\n",
        "    print('hypothesis:', x1)\n",
        "    print('label:', y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3SYhBk65mvk"
      },
      "source": [
        "# BERT 微调 - 开始微调 BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 通常来说，不固定预训练模型的参数\n",
        "- BERT tokenization, 可以完全搬到 C++\n",
        "- BERT 开销非常多，文本做很大，图片往往不大\n",
        "- TODO:BERT distillation BERT **蒸馏**\n",
        "  - 1/10 大小，精度不会损失太多"
      ],
      "metadata": {
        "id": "CQff5CZxTG1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O25ui6L26Nui"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import multiprocessing\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRuiPqFC5fxT"
      },
      "outputs": [],
      "source": [
        "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
        "                            '225d66f04cae318b841a13d32af3acc165f253ac')\n",
        "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
        "                            'c72329e68a732bef0452e4b96a1c341c8910f81f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJhO1ccy5g-k"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
        "                          num_heads, num_layers, dropout, max_len, devices):\n",
        "    data_dir = d2l.download_extract(pretrained_model)\n",
        "    # 定义空词表以加载预定义词表\n",
        "    vocab = d2l.Vocab()\n",
        "    vocab.idx_to_token = json.load(open(os.path.join(data_dir,\n",
        "        'vocab.json')))\n",
        "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
        "        vocab.idx_to_token)}\n",
        "    bert = d2l.BERTModel(len(vocab), num_hiddens,\n",
        "                         ffn_num_hiddens=ffn_num_hiddens,\n",
        "                         num_heads=4, dropout=0.2, num_blks=2,\n",
        "                         max_len=max_len)\n",
        "    # 加载预训练BERT参数\n",
        "    bert.load_state_dict(torch.load(os.path.join(data_dir,\n",
        "                                                 'pretrained.params')))\n",
        "    return bert, vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "devices = d2l.try_all_gpus()\n",
        "bert, vocab = load_pretrained_model(\n",
        "    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n",
        "    num_layers=2, dropout=0.1, max_len=512, devices=devices)"
      ],
      "metadata": {
        "id": "Zr05vBYcH4pB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adc746e-9dcd-4dfb-94f0-b9d72c200b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ../data/bert.small.torch.zip from http://d2l-data.s3-accelerate.amazonaws.com/bert.small.torch.zip...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HMAXRi2NrMU",
        "outputId": "f22b8828-b279-4a6e-d2da-9445d693fe15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTModel(\n",
              "  (encoder): BERTEncoder(\n",
              "    (token_embedding): Embedding(60005, 256)\n",
              "    (segment_embedding): Embedding(2, 256)\n",
              "    (blks): Sequential(\n",
              "      (0): TransformerEncoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (W_q): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "          (W_k): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "          (W_v): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "          (W_o): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "        )\n",
              "        (addnorm1): AddNorm(\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (ffn): PositionWiseFFN(\n",
              "          (dense1): LazyLinear(in_features=0, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dense2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "        )\n",
              "        (addnorm2): AddNorm(\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (1): TransformerEncoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (attention): DotProductAttention(\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (W_q): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "          (W_k): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "          (W_v): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "          (W_o): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "        )\n",
              "        (addnorm1): AddNorm(\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (ffn): PositionWiseFFN(\n",
              "          (dense1): LazyLinear(in_features=0, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dense2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "        )\n",
              "        (addnorm2): AddNorm(\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (hidden): Sequential(\n",
              "    (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "    (1): Tanh()\n",
              "  )\n",
              "  (mlm): MaskLM(\n",
              "    (mlp): Sequential(\n",
              "      (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (3): LazyLinear(in_features=0, out_features=60005, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (nsp): NextSentencePred(\n",
              "    (output): LazyLinear(in_features=0, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "6HG7JhYVN0dK",
        "outputId": "5e5fdaee-8118-46e3-cf07-9c93915317c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<d2l.torch.Vocab at 0x7a8b6c6c8e20>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8UN8k_h6X8u"
      },
      "source": [
        "- The Dataset for Fine-Tuning BERT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg8JXDEE5zOE"
      },
      "outputs": [],
      "source": [
        "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, max_len, vocab=None):\n",
        "        all_premise_hypothesis_tokens = [[\n",
        "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
        "            *[d2l.tokenize([s.lower() for s in sentences])\n",
        "              for sentences in dataset[:2]])]\n",
        "\n",
        "        self.labels = torch.tensor(dataset[2])\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "        (self.all_token_ids, self.all_segments,\n",
        "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
        "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
        "\n",
        "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
        "        pool = multiprocessing.Pool(4) # 4个多进程，BERT 数据预处理很慢； Python本身多线程；CV里 Operator 都是 C++写的；\n",
        "        # python 写 多线程； OR 写 C++ 代码\n",
        "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
        "        all_token_ids = [\n",
        "            token_ids for token_ids, segments, valid_len in out]\n",
        "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
        "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
        "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
        "                torch.tensor(all_segments, dtype=torch.long),\n",
        "                torch.tensor(valid_lens))\n",
        "\n",
        "    def _mp_worker(self, premise_hypothesis_tokens):\n",
        "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
        "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
        "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
        "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
        "                             * (self.max_len - len(tokens))\n",
        "        segments = segments + [0] * (self.max_len - len(segments))\n",
        "        valid_len = len(tokens)\n",
        "        return token_ids, segments, valid_len\n",
        "\n",
        "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
        "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
        "            if len(p_tokens) > len(h_tokens):\n",
        "                p_tokens.pop()\n",
        "            else:\n",
        "                h_tokens.pop()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
        "                self.valid_lens[idx]), self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_token_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELDQEdco6qp9"
      },
      "source": [
        "Generate training and testing examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bpTCevJ6PZk",
        "outputId": "6212b9bd-3f5c-4568-c2ad-073d37233517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read 549367 examples\n",
            "read 9824 examples\n"
          ]
        }
      ],
      "source": [
        "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
        "data_dir = d2l.download_extract('SNLI')\n",
        "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
        "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
        "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n",
        "                                   num_workers=num_workers)\n",
        "test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
        "                                  num_workers=num_workers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVUmjX1S6v-F"
      },
      "source": [
        "BERT tokenization == 词根 tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SNLIDataset(torch.utils.data.Dataset): # 需要允许传进来BERT传入的Vocab\n",
        "    \"\"\"用于加载SNLI数据集的自定义数据集\"\"\"\n",
        "    def __init__(self, dataset, num_steps, vocab=None):\n",
        "        self.num_steps = num_steps\n",
        "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
        "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
        "        if vocab is None:\n",
        "            self.vocab = d2l.Vocab(all_premise_tokens +\n",
        "                all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "        self.premises = self._pad(all_premise_tokens)\n",
        "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
        "        self.labels = torch.tensor(dataset[2])\n",
        "        print('read ' + str(len(self.premises)) + ' examples')\n",
        "\n",
        "    def _pad(self, lines):\n",
        "        return torch.tensor([d2l.truncate_pad(\n",
        "            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n",
        "                         for line in lines])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.premises)"
      ],
      "metadata": {
        "id": "w6ATpJsZ7Af9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_snli(batch_size, num_steps=50):\n",
        "    \"\"\"下载SNLI数据集并返回数据迭代器和词表\"\"\"\n",
        "    num_workers = d2l.get_dataloader_workers()\n",
        "    data_dir = d2l.download_extract('SNLI')\n",
        "    train_data = read_snli(data_dir, True)\n",
        "    test_data = read_snli(data_dir, False)\n",
        "    train_set = SNLIDataset(train_data, num_steps)\n",
        "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
        "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=num_workers)\n",
        "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
        "                                            shuffle=False,\n",
        "                                            num_workers=num_workers)\n",
        "    return train_iter, test_iter, train_set.vocab"
      ],
      "metadata": {
        "id": "JHcX8_xU7OLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGA2IbEo7Qsf",
        "outputId": "4ce9350e-d85b-4e49-a8d2-3c013989114f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read 549367 examples\n",
            "read 9824 examples\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18678"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for X, Y in train_iter:\n",
        "    print(X[0].shape)\n",
        "    print(X[1].shape)\n",
        "    print(Y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ybsELU47UCQ",
        "outputId": "1d9290c2-f54d-4367-becd-23d4bb99ec18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 50])\n",
            "torch.Size([128, 50])\n",
            "torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for X, Y in train_iter:\n",
        "    print(X[0].shape)\n",
        "    print(X[1].shape)\n",
        "    print(Y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YxHC56R7WTs",
        "outputId": "07abff21-c55f-4d93-9a39-e3938b209f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 50])\n",
            "torch.Size([128, 50])\n",
            "torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiSWZNgq74_t"
      },
      "source": [
        " This MLP transforms the BERT representation of the special “<cls>” token into three outputs of natural language inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3vhuiX67xoM"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.encoder = bert.encoder\n",
        "        self.hidden = bert.hidden\n",
        "        self.output = nn.Linear(256, 3) # 3个类\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        tokens_X, segments_X, valid_lens_x = inputs\n",
        "        encoded_X = self.encoder(\n",
        "            tokens_X, segments_X, valid_lens_x)\n",
        "        return self.output(self.hidden(\n",
        "            encoded_X[:, 0, :]))# 拿到第一个hidden\n",
        "\n",
        "net = BERTClassifier(bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training"
      ],
      "metadata": {
        "id": "WIllD0q5SR3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ry5th8461Ug"
      },
      "outputs": [],
      "source": [
        "# from d2l import torch as d2l\n",
        "# # train_ch13 方法中，增加 backend_inline.set_matplotlib_formats('png')\n",
        "# from matplotlib_inline import backend_inline\n",
        "# backend_inline.set_matplotlib_formats('png')\n",
        "\n",
        "# lr, num_epochs = 1e-4, 5\n",
        "# trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "# loss = nn.CrossEntropyLoss(reduction='none')\n",
        "# d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZfswwumZU6WR",
        "bUGXHdEkU13D",
        "yRMP6gvhpdWH",
        "s4NJaUs8qnCU",
        "uDCIOBijrOsi"
      ],
      "authorship_tag": "ABX9TyNKHN4GEojq1kvafN/Uk6Vu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}